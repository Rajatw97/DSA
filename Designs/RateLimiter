Q: Design rate limiter? 

A brief about Rate limiter: 
It is used to limit the number of requests that a particular client can send to the backend server. The need for it is to to ensure that the traffic in the backend 
server doesn't exceed some threshold whic may cause system failure. Now there can be multiple different scenarios which would require to be handled differently like
For eg: lets say a product went viral so we will see a huge spike in number of requests and it would be needed to handle gracefully or someone could be trying 
something malicious like launchind DDOS attack to bring down the server, so these needs to be handled differently. Also, The rate at which the traffic comes 
is not in our control, at any point of time we should be prepared to see a burst or spike in traffic. Using a database is not a good idea due to slowness of 
disk access. In memory cache is the way to go. Because it is quick and supports a time-based expiration technique, an in-memory cache is chosen for rate limiter.

Now there are various algorithms with its own pros & cons which is used to implement rate limiter:

1. Token bucket :
Token bucket is a container that has pre-defined capacity. Tokens are put in the bucket at fixed rates periodically. Once the bucket is full, no more tokens are added.
Pros:  
a. Very simple and easy to implement, memory efficient.
b. Allows spike in traffic or burst of traffic,  A request goes through as long as there are tokens left.
Cons: 
a. Fine tuning of params
Usecase: Stripe, EC2
Redis: <user_id, {tokens, timestamp}> : Here tokens represent no of tokens left, init with max defined.
Concurrency issue: Token bucketâ€™s Redis operations atomic via Lua scripting.

2. Leaky bucket :
Leaky bucket has a bucket with a finite capacity for each client. It uses a limited sized queue and process requests at a constant rate from queue in FIFO manner. 
In case of request overflow, new requests are dropped. 
Pros:
a) Memory efficient given the limited queue size
b) Requests are processed at a fixed rate, therefore it is suitable for use cases where a stable outflow rate id required.
Cons:
1) A burst of traffic fills up the queue with old requests, and if they are not processed in time, recent requests will be rate limited.

3. Fix window:

It divides the timeline into fixed-size windows and assign a counter to each window. Each request, based on its arriving time, is mapped to a window.
If the counter in the window has reached the limit, requests falling in this window should be rejected.
Pros:
a)  It ensures that most recent requests are served without being starved by old requests.
Cons:
a) At edge of a window, it can allow twice number of requests causing overload on server.
Redis:  <userid_timestamp, no of req made> : Already atomic so no concurrency

4. Sliding window:
It is used to overcome the problem that we face in fixed window by instead of defining fixed window intervals, we keep sliding the intervals so that we check
for reuqests in past minute only from whne the request came.
Pros:
a) Works very accurately
Cons:
b)  Consumes a lot of memorybecause even if a request is rejected , it's timestamp will still be stored in memory.
Redis: Timestamp data is kept in sorted sets of Redis.

5) Sliding window Counter:  
A hybrid approach that combines the Fixed Window Counter algorithm and Sliding Window Logs algorithm.
Formulae: Requests in current window + (Requests in the previous window * overlap percentage of the rolling window and previous window)
Pros:
a) Memory efficient. It smoothes out spikes in the traffic because the rate is based on the average rate of the previous window.
Cons:
a) It is an approximation of the actual rate because it assumes requests in the previous window are evenly distributed.

Implementation:

public interface RateLimiter {
    boolean grantAccess();
}
public class TokenBucket implements RateLimiter {
    private int bucketCapacity;
    private int refreshRate;
    private AtomicInteger currentCapacity;
    private AtomicLong lastUpdatedTime;

    public TokenBucket(int bucketCapacity, int refreshRate) {
        this.bucketCapacity = bucketCapacity;
        this.refreshRate = refreshRate;
        currentCapacity = new AtomicInteger(bucketCapacity);
        lastUpdatedTime = new AtomicLong(System.currentTimeMillis());
    }

    @Override
    public boolean grantAccess() {
        refreshBucket();
        if(currentCapacity.get() > 0){
            currentCapacity.decrementAndGet();
            return true;
        }
        return false;
    }

    void refreshBucket(){
        long currentTime = System.currentTimeMillis();
        int additionalToken = (int) ((currentTime-lastUpdatedTime.get())/1000 * refreshRate);
        int currCapacity = Math.min(currentCapacity.get()+additionalToken, bucketCapacity);
        currentCapacity.getAndSet(currCapacity);
        lastUpdatedTime.getAndSet(currentTime);
    }
}

public class LeakyBucket implements RateLimiter {
    BlockingQueue<Integer> queue;

    public LeakyBucket(int capacity) {
        this.queue = new LinkedBlockingQueue<>(capacity);
    }

    @Override
    public boolean grantAccess() {
        if(queue.remainingCapacity() > 0){
            queue.add(1);
            return true;
        }
        return false;
    }
}

public class SlidingWindow implements RateLimiter {
    Queue<Long> slidingWindow;
    int timeWindowInSeconds;
    int bucketCapacity;

    public SlidingWindow(int timeWindowInSeconds, int bucketCapacity) {
        this.timeWindowInSeconds = timeWindowInSeconds;
        this.bucketCapacity = bucketCapacity;
        slidingWindow = new ConcurrentLinkedQueue<>();
    }

    @Override
    public boolean grantAccess() {
        long currentTime = System.currentTimeMillis();
        checkAndUpdateQueue(currentTime);
        if(slidingWindow.size() < bucketCapacity){
            slidingWindow.offer(currentTime);
            return true;
        }
        return false;
    }

    private void checkAndUpdateQueue(long currentTime) {
        if(slidingWindow.isEmpty()) return;

        long calculatedTime = (currentTime - slidingWindow.peek())/1000;
        while(calculatedTime >= timeWindowInSeconds){
            slidingWindow.poll();
            if(slidingWindow.isEmpty()) break;
            calculatedTime = (currentTime - slidingWindow.peek())/1000;
        }
    }
}

public class UserBucketCreator {
    Map<Integer, RateLimiter> bucketMap;

    public UserBucketCreator() {
        this.bucketMap = new HashMap<>();
    }

    public void addUserBucket(int userId, RateLimiter rateLimiter) {
        bucketMap.put(userId, rateLimiter);
    }

    public void accessApplication(int userId) {
        RateLimiter rateLimiter = bucketMap.get(userId);
        if(rateLimiter.grantAccess()){
            System.out.println(Thread.currentThread().getName() + " -> able to access the application");
        } else {
            System.out.println(Thread.currentThread().getName() + " -> Too many request, Please try after some time");
        }
    }
}

public class Appl {
    public static void main(String[] args) {
    
    UserBucketCreator userBucketCreator = new UserBucketCreator();
    userBucketCreator.addUserBucket(1, new TokenBucket(10, 10));
    userBucketCreator.addUserBucket(2, new LeakyBucket(10));
    userBucketCreator.addUserBucket(3, new SlidingWindow(10, 10));

    ExecutorService executors = Executors.newFixedThreadPool(12);
    for (int i = 0; i < 12; i++) {
        executors.execute(() -> {
            int userId = (int) (Math.random() * 3) + 1;
            userBucketCreator.accessApplication(userId);
        });
    }
    executors.shutdown();
    }
}


Alternate design that includes ip/id:

public class Main {

    public static void main(String[] args) {

        UserBucketCreator userBucketCreator = new UserBucketCreator();
        userBucketCreator.addUserBucket("192.168.1.1", new TokenBucket(4, 10));
        userBucketCreator.addUserBucket("192.168.1.2", new TokenBucket(4,10));
        userBucketCreator.addUserBucket("user1", new SlidingWindow(5, 10));

        ExecutorService executors = Executors.newFixedThreadPool(12);
        for (int i = 0; i < 12; i++) {
            executors.execute(() -> {
                int reandomKey = (int) (Math.random() * 12) + 1;
                String key = reandomKey < 4 ? "192.168.1.1" : reandomKey < 8 ? "192.168.1.2" : "user1";
                userBucketCreator.accessApplication(key);
            });
        }
        executors.shutdown();
    }
}

class UserBucketCreator {
Map<String, RateLimiter> bucketMap; // Using a generic String key to support both IP address and User ID cases

public UserBucketCreator() {
    this.bucketMap = new HashMap<>();
}

public void addUserBucket(String key, RateLimiter rateLimiter) {
    bucketMap.put(key, rateLimiter);
}

public void accessApplication(String key) {
    RateLimiter rateLimiter = bucketMap.get(key);
    if (rateLimiter.grantAccess(key)) {
        System.out.println(Thread.currentThread().getName() + " -> able to access the application");
    } else {
        System.out.println(Thread.currentThread().getName() + " -> Too many requests, please try after some time");
    }
 }
}

 interface RateLimiter {
    boolean grantAccess(String key); // Key can be either IP address or User ID depending on the implementation
}

 class TokenBucket implements RateLimiter {
    private int bucketCapacity;
    private int refreshRate;
    private Map<String, Bucket> buckets; // We need to maintain separate buckets for each key (IP address or User ID)

    public TokenBucket(int bucketCapacity, int refreshRate) {
        this.bucketCapacity = bucketCapacity;
        this.refreshRate = refreshRate;
        buckets = new ConcurrentHashMap<>();
    }

    @Override
    public boolean grantAccess(String key) {
        refreshBucket(key);
        Bucket bucket = buckets.get(key);
        if(bucket.currentCapacity.get() > 0){
            bucket.currentCapacity.decrementAndGet();
            return true;
        }
        return false;
    }

    //when one thread is updating the currentCapacity the other thread can read the last updatedTime, it can result in race condition. so using synchronized
    synchronized void refreshBucket(String key){
        Bucket bucket = buckets.get(key);
        if(bucket == null){
            bucket = new Bucket();
            buckets.put(key, bucket);
        }
        long currentTime = System.currentTimeMillis();
        int additionalToken = (int) ((currentTime-bucket.lastUpdatedTime.get())/1000 * refreshRate);
        int currCapacity = Math.min(bucket.currentCapacity.get()+additionalToken, bucketCapacity);
        bucket.currentCapacity.getAndSet(currCapacity);
        bucket.lastUpdatedTime.getAndSet(currentTime);
    }

    private class Bucket {
        private AtomicInteger currentCapacity;
        private AtomicLong lastUpdatedTime;

        public Bucket() {
            currentCapacity = new AtomicInteger(bucketCapacity);
            lastUpdatedTime = new AtomicLong(System.currentTimeMillis());
        }
    }
}

 class SlidingWindow implements RateLimiter {
    Map<String, Queue<Long>> slidingWindows; // We need to maintain separate sliding windows for each key (IP address or User ID)
    int timeWindowInSeconds;
    int bucketCapacity;

    public SlidingWindow(int timeWindowInSeconds, int bucketCapacity) {
        this.timeWindowInSeconds = timeWindowInSeconds;
        this.bucketCapacity = bucketCapacity;
        slidingWindows = new ConcurrentHashMap<>();
    }

    @Override
    public boolean grantAccess(String key) {
        long currentTime = System.currentTimeMillis();
        checkAndUpdateQueue(currentTime, key);
        Queue<Long> slidingWindow = slidingWindows.get(key);
        if(slidingWindow.size() < bucketCapacity){
            slidingWindow.offer(currentTime);
            return true;
        }
        return false;
    }

    private void checkAndUpdateQueue(long currentTime, String key) {
        Queue<Long> slidingWindow = slidingWindows.get(key);
        if(slidingWindow == null){
            slidingWindow = new ConcurrentLinkedQueue<>();
            slidingWindows.put(key, slidingWindow);
        }
        if(slidingWindow.isEmpty()) return;

        long calculatedTime = (currentTime - slidingWindow.peek())/1000;
        while(calculatedTime >= timeWindowInSeconds){
            slidingWindow.poll();
            if(slidingWindow.isEmpty()) break;
            calculatedTime = (currentTime - slidingWindow.peek())/1000;
        }
    }
}

Additional:

1) Api crashes at 1000 request per second. How do I determine the rate at which the requests are prevented. 1000? 800? and why.
     Run experiments as well as load tests to have a deterministic rate at which requests are dropped. We should also use that information to apply global rate limit
     on the application. So, if the rate determined after our experiments and load test is 1000 requests / second, we can cap global rate limit at 800.
    
2) Api has 1:3 requests for POST and GET. How to determine rate limit with this information.
    This means that if the rate limit for POST requests is 200 requests / second, GET should be 600 requests / second.
    
3) Where to deploy the rate limiter? How to reduce network calls?
    We can deploy the rate limiter as part of the API gateway.
    To reduce network calls, we can cache the GET requests on the client. This however, introduces some complexities. We need to figure out how to ensure that if
    there is a change in the data, clients will discard their caches and go back to the server to fetch the updated records. If we know the rate at which data changes
    occur, then we can build it into our implementation. One way to tackle this is to invalidate client cache say every 3 seconds. Each time client has need to serve
    data, it first checks if the cache is still valid. If the cache is still valid and the data it's looking for is in its cache, it will serve from the cache, 
    otherwise, it will do a network call.This approach will generally reduce the number of calls the service receives but also introduces an eventual consistency
    that is within 3 seconds. 
We could resort to adding more workers to help with serving the api requests. We should also apply caching on the server side. While this will generally improve the health of our servers, it does not reduce network calls.
